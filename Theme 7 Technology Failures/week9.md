<span style = "color:red">Normalization of Deviance</span>

多员视角



# 1. Emmy-Award Winning Documentary on the Loss of Challenger

### **视频标题：**

**Emmy-Award Winning Documentary on the Loss of Challenger**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=2FehGJQlOf0) (50m21s)

------

### **核心内容概述**

这部纪录片回顾了 1986 年 **挑战者号航天飞机灾难（Challenger Disaster）** 的背景、原因以及后续影响。它深入探讨了技术失败背后的文化、组织和技术因素，为研究高风险技术的失败机制提供了宝贵的案例。

------

### **1. 挑战者号灾难概述**

#### **1.1 事件背景**

- 挑战者号于1986年1月28日发射，升空73秒后解体，导致7名宇航员全部遇难。
  The Challenger was launched on January 28, 1986, and disintegrated 73 seconds into its launch, killing all seven astronauts on board.
- 这是美国国家航空航天局（NASA）历史上最严重的灾难之一。
  It was one of the worst disasters in NASA's history.

#### **1.2 技术失败原因**

- 主因是**固体助推器密封圈（O-rings）**的失效。
  The main cause was the failure of the solid booster seals (O-rings).
  - **O-rings** 在低温环境下失去弹性，无法有效密封燃料泄漏。
- 这一技术问题早在灾难发生前已被工程师发现，但未引起足够重视。
  This technical problem had been discovered by engineers long before the disaster, but it did not receive enough attention.

#### **1.3 组织和文化问题**

- NASA 内部的**组织文化（Organizational Culture）**强调进度和成本控制，忽视了安全警告。
  The organizational culture within NASA emphasizes schedule and cost control and ignores safety warnings.
- 存在**偏差的正常化（Normalization of Deviance）**现象：
  - 小规模的技术异常未导致重大问题，因此被逐渐接受为“正常”。
    Small-scale technical anomalies did not cause major problems and were gradually accepted as "normal".

------

### **2. 与“技术失败”主题的关联**

#### **2.1 高风险技术的不可预测性（Unpredictability of High-Risk Technologies）**

- **C. Perrow 的“正常事故理论（Normal Accident Theory）”**认为：
  - 在复杂且高度耦合的技术系统中，失败是不可避免的。
    In complex and highly coupled technical systems, failure is inevitable.
  - 挑战者号的灾难是此理论的典型案例，展示了复杂系统中小故障可能触发大规模失败。
    The Challenger disaster is a classic example of this theory, demonstrating how small glitches in complex systems can trigger massive failures.

#### **2.2 人为因素的影响（Human Factors in Failures）**

- **J. Reason 的“人为贡献（The Human Contribution）”**强调：
  - 技术失败常与人的不安全行为有关，例如管理失误、决策失误和不充分的沟通。
  - 在挑战者号事件中，技术人员的警告被高层管理忽视，直接导致了灾难。

#### **2.3 文化与偏差（Culture and Deviance）**

- D. Vaughan 的研究

  分析了 NASA 的文化如何助长了风险行为：

  - **文化偏差（Cultural Deviance）：** 组织倾向于接受违规行为，因为这些行为在短期内看似无害。
  - NASA 的文化优先考虑项目时间表和预算，而非工程师提出的安全顾虑。

#### **2.4 失败中的学习（Learning from Failures）**

- H. Petroski 的观点：
  - 失败是设计成功的重要组成部分。通过分析挑战者号的失败，航天领域在后续任务中引入了更严格的质量控制和风险评估。

------

### **3. 技术失败的关键名词中英对照**

| **中文**                  | **英文**                                |
| ------------------------- | --------------------------------------- |
| 正常事故理论              | Normal Accident Theory                  |
| 高风险技术                | High-Risk Technologies                  |
| 组织文化                  | Organizational Culture                  |
| 偏差的正常化              | Normalization of Deviance               |
| 固体助推器密封圈（O形环） | Solid Rocket Booster O-Rings            |
| 人为贡献                  | The Human Contribution                  |
| 文化偏差                  | Cultural Deviance                       |
| 风险评估                  | Risk Assessment                         |
| 故障模式与影响分析        | Failure Mode and Effect Analysis (FMEA) |

------

### **4. 对本主题的深远影响**

#### **4.1 技术的不可预测性**

- 挑战者号的灾难表明，即使在高度工程化的系统中，也难以完全消除失败的可能性。
- 强调了在复杂技术系统中建立冗余和持续监测的重要性。

#### **4.2 文化的重要性**

- 组织文化在风险管理中至关重要。
  Organizational culture is crucial in risk management.
- 灾难凸显了打破“沉默文化（Culture of Silence）”的必要性，确保技术人员的声音能够被高层管理听到。
  The disaster highlighted the need to break the "culture of silence" and ensure the voices of technical staff are heard by senior management.

#### **4.3 从失败中学习**

- 灾难引发了对 NASA 组织文化的深刻反思，推动了整个航天行业的改革。
  The disaster triggered a profound reflection on NASA’s organizational culture and promoted the reform of the entire aerospace industry
- 更广泛地，挑战者号事件教导我们在设计和管理高风险技术时，需要更全面的风险评估和沟通机制。
  More broadly, the Challenger incident taught us that more comprehensive risk assessment and communication mechanisms are needed when designing and managing high-risk technologies.

------

### **总结**

挑战者号灾难是研究技术失败的经典案例，其背后的技术、文化和组织因素提供了深刻的教训。通过理解高风险技术的复杂性和不可预测性，结合文化与人为因素的分析，我们可以更好地预防未来的灾难，同时从失败中学习，改进技术和管理方法。



# 2. Columbia

### **视频标题：**

**Documentary on the Loss of Columbia**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=gWhqLVkjK50) (44m31s)

------

### **核心内容概述**

这部纪录片回顾了 2003 年 **哥伦比亚号航天飞机灾难（Columbia Disaster）** 的背景、技术细节以及影响，展示了技术和组织失误如何共同导致高风险技术的失败。与**挑战者号灾难**一样，这次事件成为研究复杂技术系统失败机制的重要案例。

------

### **1. 哥伦比亚号灾难概述**

#### **1.1 事件背景**

- **日期：** 2003年2月1日。
- **事件：** 在执行 STS-107 任务期间，哥伦比亚号在重返地球时解体，7名宇航员遇难。
- **技术问题：** 发射时，一个隔热泡沫块脱落，撞击航天飞机的左翼，造成热保护系统受损。重返大气层时，高温气流进入机体内部，最终导致解体。

#### **1.2 组织与文化因素**

- NASA 再次被指出存在

  组织文化问题（Organizational Culture Issues）：

  - 早期警告未被充分重视。
    Early warnings were not adequately heeded.
  - 决策层未能进行更深入的风险评估。
    Decision-makers failed to conduct a more in-depth risk assessment.

------

### **2. 与技术失败主题的关联**

#### **2.1 高风险技术中的复杂性（Complexity in High-Risk Technologies）**

- **C. Perrow 的“正常事故理论”**认为：
  - 在高度复杂和紧密耦合的技术系统中，失败并非异常，而是一种不可避免的“正常现象”。
    In highly complex and tightly coupled technical systems, failure is not an anomaly but an inevitable “normal phenomenon”.
  - 哥伦比亚号的灾难再次验证了这一理论，即使微小的初始问题（泡沫脱落）也能导致系统性崩溃。
    The Columbia disaster reaffirmed the theory that even a minor initial problem (bubble shedding) could lead to a systemic collapse.

#### **2.2 人为因素的角色（Role of Human Factors）**

- J. Reason 的“人为贡献”：
  - 认为事故不仅仅源于技术故障，还与人的行为密切相关。
    It is believed that accidents are not only caused by technical failures, but are also closely related to human behavior.
  - 在哥伦比亚号事件中，技术人员提出了对泡沫脱落的担忧，但被决策者忽略或低估。
    In the Columbia incident, technicians raised concerns about foam shedding, but these concerns were ignored or underestimated by decision makers.

#### **2.3 文化偏差与组织行为（Cultural Deviance and Organizational Behavior）**

- D. Vaughan 的研究：
  - 指出 NASA 的组织文化存在长期隐患，例如**偏差的正常化（Normalization of Deviance）**。
  - Points out that NASA's organizational culture has long-term problems, such as the normalization of deviance.
  - 泡沫脱落问题在多次任务中已被观察到，但因未立即引发灾难而被接受为“常规现象”。
    The foam shedding problem had been observed on multiple missions but was accepted as a "routine phenomenon" because it did not cause an immediate disaster.

#### **2.4 决策中的信息不对称（Information Asymmetry in Decision-Making）**

- 技术团队和管理层之间的沟通不畅导致风险未被正确评估。
  Poor communication between the technical team and management resulted in risks not being properly assessed.
- **W. Starbuck 和 M. Farjoun** 的研究指出，这种信息鸿沟是复杂组织中常见的问题。

------

### **3. 技术失败的关键名词中英对照**

| **中文**     | **英文**                        |
| ------------ | ------------------------------- |
| 正常事故理论 | Normal Accident Theory          |
| 偏差的正常化 | Normalization of Deviance       |
| 高风险技术   | High-Risk Technologies          |
| 组织文化     | Organizational Culture          |
| 人为贡献     | The Human Contribution          |
| 信息不对称   | Information Asymmetry           |
| 风险评估     | Risk Assessment                 |
| 热保护系统   | Thermal Protection System (TPS) |

------

### **4. 从哥伦比亚号灾难中学习的关键点**

#### **4.1 技术与组织的交互（Interplay of Technology and Organization）**

- 技术故障和组织行为的结合导致了灾难。
- 强调了技术系统的维护需要强有力的组织支持。

#### **4.2 风险管理中的文化变革（Cultural Change in Risk Management）**

- 灾难后的 NASA 进行了深刻的文化反思，包括改进沟通机制和强化风险评估。
- 确保所有层级对技术问题的担忧能够得到充分表达和解决。

#### **4.3 系统性学习的重要性（Importance of Systemic Learning）**

- 通过灾难的调查报告（如哥伦比亚号事故调查委员会报告），航天工业在技术、组织和文化方面得到了宝贵的教训。

#### **4.4 冗余与弹性（Redundancy and Resilience）**

- 建议在复杂技术系统中引入更多的冗余设计和测试，增强系统的容错能力。

------

### **5. 对现代技术领域的启示**

#### **5.1 应用到人工智能和数据科学**

- 在现代复杂技术（如人工智能和数据系统）中，类似的问题可能以不同形式出现：
  - 算法偏差（Algorithmic Bias）。
  - 数据隐私风险（Data Privacy Risks）。
  - 决策自动化中的信息不透明（Opacity in Automated Decision-Making）。
    Information opacity in policy automation

#### **5.2 跨领域的风险管理**

- 哥伦比亚号的教训可以应用于其他高风险领域，例如核能、医疗技术和金融系统。

------

### **总结**

哥伦比亚号航天飞机的灾难揭示了复杂技术系统的脆弱性以及组织文化对风险管理的深远影响。通过理解技术和组织行为之间的交互机制，我们可以更好地设计高风险技术系统并管理其潜在风险。它是“技术失败”主题下的重要案例，强调了在技术、文化和组织层面进行系统性学习和改进的必要性。



# 3.Allan McDonald是 Morton Thiokol（该公司为 NASA 提供航天飞机固体火箭助推器的公司）航天飞机固体火箭项目的前总监

### **视频标题：**

**Allan McDonald on the Challenger Loss**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=QbtY_Wl-hYI) (20m28s)

------

### **核心内容概述**

在这段讲话中，Allan McDonald 作为摩顿提奥科尔公司（Morton Thiokol）固体火箭推进器项目的负责人，分享了他对**挑战者号灾难（Challenger Disaster）**的回忆和洞察。他强调了技术风险、组织决策失误和伦理责任的重要性，揭示了灾难背后的复杂因素。

------

### **1. 挑战者号灾难的技术与组织背景**

#### **1.1 技术问题**

- 核心技术问题：固体火箭推进器的 O 形密封圈（O-rings）失效
  - 低温失效（Low-Temperature Failure）：
    - 发射当天的温度异常低（摄氏-2°C），导致密封圈弹性下降。
    - 密封圈无法有效密封，导致燃料泄漏并最终引发爆炸。

#### **1.2 决策问题**

- McDonald 的反对：
  - McDonald 和其他工程师明确指出，低温环境下的发射存在严重风险。
  - 他在发射前的技术讨论中坚决主张推迟发射。
- NASA 的忽视：
  - 管理层面临时间表压力和公众期待，选择无视工程师的警告。
    Management, faced with timeline pressures and public expectations, chose to ignore the engineers’ warnings.
  - 高层决定继续发射，导致灾难发生。
    The top brass decided to go ahead with the launch, leading to the disaster.

------

### **2. 与技术失败主题的关联**

#### **2.1 高风险技术系统的不可预测性**

- C. Perrow 的“正常事故理论”：
  - 在复杂且紧密耦合的技术系统中，小问题（如 O 形密封圈失效）可以迅速升级为灾难。
    In complex and tightly coupled technical systems, small problems, such as a failed O-ring seal, can quickly escalate into a disaster.
  - 挑战者号事件是这种理论的典型案例。

#### **2.2 组织文化对风险的影响**

- 偏差的正常化（Normalization of Deviance）：
  - NASA 在之前的任务中已观察到密封圈的性能问题，但因未引发灾难而逐渐接受这些问题为“常规”。
  - McDonald 的警告未被充分重视，表明 NASA 组织文化中存在的风险管理盲点。

#### **2.3 伦理责任与技术决策**

- McDonald 的讲话突出了**技术伦理（Technological Ethics）**的重要性：
  - 工程师在科学数据和技术风险评估中承担核心责任。
    Engineers have a central role in scientific data and technology risk assessment.
  - 组织决策应以技术安全为优先，而非经济或政治压力。
    Organizational decisions should be based on technical security rather than economic or political pressures.

------

### **3. Allan McDonald 的观点与教训**

#### **3.1 科学与工程师的声音**

- 技术决策中的独立性：
  - McDonald 强调工程师在技术决策中的核心作用，认为其科学依据不应被管理层忽视。
    McDonald stressed the central role of engineers in technical decision-making, arguing that its scientific basis should not be ignored by management.
  - 他提出，如果技术问题未被充分解决，宁可冒着推迟发射的代价，也不应继续任务。

#### **3.2 决策透明度**

- McDonald 指出，NASA 决策中缺乏透明度和对异议的尊重是灾难的重要原因。

#### **3.3 技术伦理的重要性**

- 工程师不仅是技术问题的解决者，还应是技术安全和伦理的守护者。
  Engineers are not only technical problem solvers, but also guardians of technical safety and ethics.
- 他说：“我们的职责是对人命负责，而不是对时间表负责。”
  "Our responsibility is to human life, not timelines," he said."

------

### **4. 技术失败的关键名词中英对照**

| **中文**       | **英文**                   |
| -------------- | -------------------------- |
| 固体火箭推进器 | Solid Rocket Booster (SRB) |
| O 形密封圈     | O-rings                    |
| 偏差的正常化   | Normalization of Deviance  |
| 高风险技术     | High-Risk Technologies     |
| 正常事故理论   | Normal Accident Theory     |
| 技术伦理       | Technological Ethics       |
| 决策透明度     | Decision Transparency      |
| 风险评估       | Risk Assessment            |

------

### **5. 对技术失败和组织决策的深远影响**

#### **5.1 对 NASA 的反思与改进**

- 挑战者号灾难后，NASA 深刻反思并调整了其组织文化，包括：
  - 强化工程师的声音和技术评估的独立性。
  - 改善风险管理流程，特别是对发射条件的评估。

#### **5.2 对高风险技术的警示**

- McDonald 的经验强调了在复杂技术项目中，技术风险不能因管理压力或政治需求被忽视。
- 组织需要建立强有力的机制，确保技术问题得到充分解决。

#### **5.3 工程师的责任与伦理**

- 工程师不仅需要解决技术问题，还需在组织决策中捍卫安全和伦理原则。
- McDonald 的坚持凸显了工程伦理的核心价值。

------

### **总结**

Allan McDonald 的讲话不仅回顾了挑战者号灾难的技术和组织背景，还对高风险技术项目的决策过程提供了深刻反思。他强调工程师在风险管理和伦理决策中的重要角色，并呼吁在复杂技术系统中建立透明、负责的组织文化。这段讲话为“技术失败”主题提供了重要的伦理和实践教训。



# 4. Mike Mullane在给消防员的讲座中alking about *normalization of deviance* in a lecture to firefighters

### **视频标题：**

**Mike Mullane on Normalization of Deviance**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=Ljzj9Msli5o) (16m35s)

------

### **核心内容概述**

Mike Mullane 是一位曾参与多次航天飞行任务的 NASA 航天员。在这段充满激情的演讲中，他以**挑战者号灾难（Challenger Disaster）**为案例，深入探讨了**偏差的正常化（Normalization of Deviance）**这一组织行为现象，并将其与消防行业的风险管理联系起来。他以清晰且直率的方式警示听众注意这一潜在的系统性问题。

------

### **1. 偏差的正常化**

#### **1.1 定义与机制**

- 偏差的正常化（Normalization of Deviance）：
  - 指组织内逐渐接受原本被认为不安全或不规范的行为或现象，因为这些行为一开始似乎没有造成直接后果。
    Refers to the gradual acceptance within an organization of behaviors or phenomena that were originally considered unsafe or unconventional because these behaviors did not initially appear to have direct consequences.
  - 机制：
    - 初期：轻微的偏差被视为“例外”或“偶然”。
    - 中期：由于偏差没有引发灾难，逐渐被视为“正常”。
    - 最终：偏差融入文化，成为默认的操作规范。

#### **1.2 在挑战者号灾难中的表现**

- NASA 在多次发射任务中观察到**O 形密封圈（O-rings）**性能问题。
- 这些偏差虽未引发灾难，但未得到彻底解决，最终在极端低温条件下导致挑战者号的解体。

------

### **2. 核心观点与警示**

#### **2.1 偏差的正常化是组织失败的根源**

- Mullane 强调，偏差的正常化并非一夜之间发生，而是逐步积累的结果。
- 当组织屡次侥幸逃过风险时，其文化可能倾向于“容忍危险”，甚至忽略明显的警示信号。

#### **2.2 领导层的作用至关重要**

- 领导层在偏差的正常化中扮演关键角色：
  - 如果领导者未对偏差提出质疑，可能导致下属模仿和默认。
  - 领导层需以身作则，积极质疑潜在的偏差并强调对安全的绝对承诺。

#### **2.3 侥幸心理的危险**

- Mullane 用生动的语言描述了**侥幸心理（Luck-Based Culture）**的危害：
  - 如果一个错误行为未引发灾难，组织可能认为这种行为是“可接受的”。
  - 侥幸心理削弱了风险意识，最终可能酿成灾难。

------

### **3. 偏差的正常化与高风险技术**

#### **3.1 挑战者号的教训**

- 挑战者号的 O 形密封圈问题早已被技术团队发现，但被反复忽视。
- 偏差被“正常化”后，NASA 的文化未对其进行根本性的改变，最终导致灾难发生。

#### **3.2 高风险技术中的系统性问题**

- **C. Perrow 的“正常事故理论”**支持了 Mullane 的观点：
  - 高风险技术的复杂性使得小问题容易被忽略，但这些问题可能在不利条件下累积成灾难。
- **J. Reason 的“人为贡献”**也指出：
  - 偏差的正常化是复杂系统中人为失误的重要来源。

------

### **4. 技术失败的关键名词中英对照**

| **中文**     | **英文**                                              |
| ------------ | ----------------------------------------------------- |
| 偏差的正常化 | Normalization of Deviance                             |
| 高风险技术   | High-Risk Technologies                                |
| 侥幸心理     | <span style = "color : red">Luck-Based Culture</span> |
| 领导层的责任 | Leadership Accountability                             |
| 风险意识     | Risk Awareness                                        |
| 正常事故理论 | Normal Accident Theory                                |
| 人为贡献     | The Human Contribution                                |

------

### **5. 应对偏差的正常化：Mullane 的建议**

#### **5.1 建立强有力的风险文化**

- 确保安全始终是组织的核心价值，而不是被次要目标（如进度或成本）压倒。
  Ensure safety remains a core value of the organization and is not overwhelmed by secondary goals such as schedule or cost.
- 鼓励员工主动汇报问题，无论问题看起来多么微不足道。
  Encourage employees to come forward with concerns, no matter how minor they may seem.

#### **5.2 确保领导层对安全的承诺**

- 领导层需营造一个鼓励质疑和反馈的环境，避免沉默文化。
  Leadership needs to create an environment that encourages questioning and feedback and avoids a culture of silence.
- 定期对操作规范进行审查，防止偏差成为“习惯”。
  Regularly review operating procedures to prevent deviations from becoming “habits”.

#### **5.3 教育和培训**

- 通过案例分析和实战演练，帮助员工认识偏差正常化的风险。
- 使用历史事故（如挑战者号灾难）作为教材，提升风险意识。
  Use historical accidents (such as the Challenger disaster) as teaching materials to enhance risk awareness.

------

### **6. Mullane 的演讲对“技术失败”主题的贡献**

#### **6.1 将偏差正常化具体化**

- Mullane 的演讲将偏差正常化从抽象理论转化为实际案例，便于理解其产生机制和后果。

#### **6.2 强调文化的重要性**

- 他指出，技术失败不仅源于设备或系统问题，还深受组织文化和领导行为的影响。

#### **6.3 提供实践建议**

- Mullane 提出的应对策略为高风险技术领域的组织提供了具体的改进方向。

------

### **总结**

Mike Mullane 的演讲以挑战者号灾难为例，揭示了偏差正常化如何在组织中积累，最终酿成灾难。他通过深入分析这一现象的机制和后果，强调了风险文化和领导层责任的重要性，为高风险技术管理提供了宝贵的经验教训。这段讲话不仅对航天领域有深远意义，也为其他行业（如消防、医疗、核能等）的风险管理提供了重要启示。





# 5. A. Minkes (1996) 对 Vaughan 的*挑战者发射决策的评论* [同上]

### **文献标题：**

**Review of Vaughan's \*The Challenger Launch Decision\***
 **作者：** A. Minkes
 **期刊：** *Technovation*
 **出版时间：** 1996年
 **页码：** 第16卷第9期，第525-526页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/4639016479)

------

### **核心内容概述**

A. Minkes 对 Diane Vaughan 的著作 *The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA* 进行了评论。这篇书评集中分析了 Vaughan 的主要观点，尤其是她对**文化偏差（Cultural Deviance）**和**组织行为（Organizational Behavior）**在挑战者号灾难中的作用的探讨。Minkes 对 Vaughan 的研究进行了评价，指出其对技术失败背后的社会和文化因素的分析提供了深刻的洞察。

------

### **1. Vaughan 的主要观点**

#### **1.1 文化偏差与偏差的正常化**

- Vaughan 提出的核心理论是

  偏差的正常化（Normalization of Deviance）：

  - NASA 在挑战者号发射前多次观察到**O 形密封圈（O-rings）**的问题，但由于这些偏差没有引发灾难，逐渐被接受为“正常”。
  - 这种文化倾向让组织忽视了潜在风险。

#### **1.2 组织文化的角色**

- Vaughan 认为 NASA 的**高压文化（High-Pressure Culture）**在推动灾难中发挥了关键作用：
  - NASA 面临巨大的时间表压力和政治期待，优先考虑任务进度而非安全问题。
  - 内部的沟通机制未能让技术团队的警告传递到高层决策者。

#### **1.3 技术与社会的互动**

- Vaughan 提出，技术失败不仅仅是硬件故障的结果，更是社会、文化和组织因素的综合产物：
  - **社会文化偏差（Cultural Deviance）**影响了技术风险的评估和管理。
  - 决策者被迫在技术安全与组织目标之间做出权衡。

------

### **2. Minkes 的评价**

#### **2.1 Vaughan 的研究贡献**

- Minkes 称赞 Vaughan 的研究方法：
  - Vaughan 深入分析了 NASA 内部的组织文化和决策过程，将挑战者号灾难置于更广泛的社会技术背景下进行解读。
  - 她的研究提供了一个理解高风险技术失败的新框架，具有跨学科的意义。

#### **2.2 文化与技术的整合分析**

- Minkes 强调，Vaughan 的研究超越了单纯的技术分析，突出了**社会文化与技术系统的交互作用（Interaction Between Social Culture and Technological Systems）**。
- Vaughan 的研究将灾难视为文化和技术失误的共同结果，而非单一的技术问题。

#### **2.3 对“偏差正常化”理论的认可**

- Minkes 认为 Vaughan 的“偏差正常化”理论是本书的重要贡献：
  - 它帮助解释了复杂组织中风险如何被低估或忽视。
  - 这一理论在其他领域（如核能、医疗技术）同样具有广泛适用性。

------

### **3. Minkes 提出的批评与建议**

#### **3.1 组织文化分析的深度**

- 虽然 Minkes 认可 Vaughan 的研究，但他指出，Vaughan 对组织文化的分析可能仍不足以全面解释灾难的复杂性。
- 他建议未来的研究可以进一步探索外部压力（如政治和经济因素）对组织行为的影响。

#### **3.2 案例研究的广泛性**

- Minkes 认为，尽管 Vaughan 的研究集中于 NASA，但她的理论是否适用于其他行业需要进一步验证。

------

### **4. 技术失败的关键名词中英对照**

| **中文**         | **英文**                                   |
| ---------------- | ------------------------------------------ |
| 偏差的正常化     | Normalization of Deviance                  |
| 文化偏差         | Cultural Deviance                          |
| 组织文化         | Organizational Culture                     |
| 高压文化         | High-Pressure Culture                      |
| 技术与社会的互动 | Interaction Between Technology and Society |
| 风险评估         | Risk Assessment                            |
| 决策传递         | Decision Communication                     |

------

### **5. Vaughan 的研究对“技术失败”主题的贡献**

#### **5.1 对技术失败的社会文化解读**

- Vaughan 的研究表明，技术失败不仅是物理故障的结果，还受到组织文化和社会压力的深刻影响。
- 她的理论为研究其他复杂系统（如金融、医疗或核能）提供了有价值的框架。

#### **5.2 提醒组织文化的重要性**

- Minkes 强调，Vaughan 的研究警示管理者在风险决策中需要高度重视组织文化，避免侥幸心理和偏差的正常化。

#### **5.3 对工程伦理的启示**

- Vaughan 的分析揭示了技术决策中的伦理问题，突出了工程师和组织在维护安全与责任中的关键作用。

------

### **总结**

A. Minkes 的评论高度评价了 Vaughan 在 *The Challenger Launch Decision* 中提出的理论和见解，特别是“偏差正常化”和“文化偏差”的概念。Minkes 认为 Vaughan 的研究为理解高风险技术失败的文化和社会根源提供了宝贵的工具，同时也指出了这一理论应用于其他行业的潜力和局限性。通过探讨 NASA 的组织文化和决策失误，这本书为“技术失败”主题提供了深刻的伦理与管理教训。



# 6. A. Tudor (1997)对Vaughan 的*挑战者发射决策的评论*

### **文献标题：**

**Review of Vaughan's \*The Challenger Launch Decision\***
 **作者：** A. Tudor
 **期刊：** *Science, Technology, & Human Values*
 **出版时间：** 1997年
 **页码：** 第22卷第4期，第523-525页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/87635299)

------

### **核心内容概述**

A. Tudor 对 Diane Vaughan 的著作 *The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA* 进行了评论，重点探讨 Vaughan 对**文化偏差（Cultural Deviance）**和**组织行为（Organizational Behavior）**在挑战者号灾难中的分析。Tudor 认为 Vaughan 的研究对理解技术失败与社会文化因素的关系提供了深刻洞见，同时也指出了一些理论和方法上的局限性。

------

### **1. Vaughan 的主要观点**

#### **1.1 偏差的正常化（Normalization of Deviance）**

- Vaughan 提出，NASA 在挑战者号发射前的决策中逐渐接受了**O 形密封圈（O-rings）**的问题，将其视为“正常”偏差。
- 核心机制：
  - 每次未导致灾难的偏差都增加了组织对问题的容忍度，最终形成了文化上的默认接受。

#### **1.2 技术与组织文化的互动**

- Vaughan 将挑战者号灾难解释为技术、文化和组织之间复杂互动的产物。
- NASA 的**高压文化（High-Pressure Culture）**导致管理层优先考虑任务进度，而非工程师提出的安全警告。

#### **1.3 社会文化视角**

- Vaughan 运用社会学方法分析了**文化偏差（Cultural Deviance）**，表明灾难并非单一技术问题，而是组织行为模式的结果。

------

### **2. Tudor 的评价**

#### **2.1 对 Vaughan 理论的认可**

- Tudor 赞扬 Vaughan 在文化与技术研究领域的贡献：
  - Vaughan 的研究扩展了对复杂技术系统失败的理解，将其视为社会文化过程的结果。
  - 她的理论帮助解释了如何在高风险技术环境中形成危险的决策文化。

#### **2.2 社会学视角的创新性**

- Tudor 特别指出，Vaughan 运用了深厚的社会学理论来解读技术失败：
  - Vaughan 强调了组织中的微观文化和宏观社会结构如何共同影响风险决策。
  - 这种视角为研究技术与社会关系的学者提供了新的方法。

------

### **3. Tudor 提出的批评与建议**

#### **3.1 方法论上的局限**

- Tudor 指出，尽管 Vaughan 的研究深入，但其依赖的**定性方法**可能难以验证某些结论的普遍性。
- 建议未来研究结合**定量分析**，以更好地验证“偏差正常化”的理论。

#### **3.2 对外部压力的分析不足**

- Tudor 认为，Vaughan 对 NASA 内部文化的分析十分细致，但对外部因素（如政治和经济压力）如何影响决策的探讨不够深入。
- NASA 在挑战者号事件中受到的公众和政府压力可能是关键背景，但未被充分研究。

#### **3.3 应用范围的局限**

- Tudor 质疑 Vaughan 的理论是否可以直接应用于其他领域的高风险技术管理。
- 建议未来研究扩展“偏差正常化”理论的适用范围。

------

### **4. 技术失败的关键名词中英对照**

| **中文**         | **英文**                                   |
| ---------------- | ------------------------------------------ |
| 偏差的正常化     | Normalization of Deviance                  |
| 文化偏差         | Cultural Deviance                          |
| 高压文化         | High-Pressure Culture                      |
| 组织文化         | Organizational Culture                     |
| 技术与社会的互动 | Interaction Between Technology and Society |
| 社会学方法       | Sociological Approach                      |
| 风险评估         | Risk Assessment                            |

------

### **5. Vaughan 的研究对“技术失败”主题的贡献**

#### **5.1 技术失败的文化解释**

- Vaughan 的理论强调，技术失败不仅是工程问题，更是社会和文化现象：
  - NASA 的文化背景和组织行为直接影响了技术问题的处理方式。

#### **5.2 提供跨学科的研究框架**

- Tudor 认可 Vaughan 的研究为技术失败研究引入了社会学视角，将技术问题置于更广泛的文化背景下进行分析。

#### **5.3 对实践的启示**

- Vaughan 的分析对高风险技术领域的组织管理提出了具体建议：
  - 需要建立更严格的风险评估机制，避免侥幸心理。
    A more stringent risk assessment mechanism needs to be established to avoid taking chances.
  - 领导层需培养对技术问题的敏感性，确保工程师的声音被听到。
    Leadership needs to develop sensitivity to technical issues and ensure that engineers’ voices are heard.

------

### **总结**

A. Tudor 在评论中高度评价了 Vaughan 的研究，认为其通过文化与技术互动的视角，为理解挑战者号灾难提供了深刻的洞见，同时对技术失败的社会文化研究做出了重要贡献。然而，Tudor 也指出 Vaughan 的理论在方法和应用上的局限性，并呼吁进一步扩展研究范围，以验证其普遍适用性。这篇评论对“技术失败”主题的研究具有重要参考价值，强调了文化和社会因素在风险管理中的核心作用。



# 7. M. Farjoun & W. Starbuck (2007) Organizing At and Beyond the Limits. [有一处新的]

### **文献标题：**

**"Organizing At and Beyond the Limits"**
 **作者：** M. Farjoun & W. Starbuck
 **期刊：** *Organization Studies*
 **出版时间：** 2007年
 **卷号与页码：** 第28卷第4期，第541-566页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/440380917)

------

### **核心内容概述**

这篇文章探讨了组织如何在面对复杂性、高风险和不确定性时运作，同时也关注了组织如何在**极限边界（Limits）\**内外进行调整与适应。Farjoun 和 Starbuck 通过理论分析和案例研究（包括\**挑战者号灾难（Challenger Disaster）\**和\**哥伦比亚号灾难（Columbia Disaster）**），揭示了组织在极端条件下的行为模式及其后果。

------

### **1. 核心主题**

#### **1.1 极限管理（Organizing at the Limits）**

- 定义：
  - 极限管理是指组织在资源有限、压力巨大和不确定性增加的情况下进行操作。
- 关键特征：
  - 高复杂性（High Complexity）：组织需要处理大量信息和动态因素。
  - 高耦合性（Tight Coupling）：系统中的各部分相互依赖，小问题可能迅速蔓延。

#### **1.2 超越极限（Organizing Beyond the Limits）**

- 定义：
  - 当组织面临超越其能力或预期的挑战时，如何调整结构、流程和文化以适应新环境。
    When an organization faces challenges that exceed its capabilities or expectations, how to adapt its structure, processes, and culture to the new environment.
- 挑战：
  - 组织需要快速调整，同时避免陷入资源耗尽或灾难性失败。

------

### **2. 文章的核心观点**

#### **2.1 复杂性与耦合性增加失败风险**

- **复杂性（Complexity）

  和

  紧密耦合性（Tight Coupling）**是高风险系统中失败的根本原因：

  - 系统越复杂，问题越难预测。
  - 耦合性越高，局部问题越容易演变为系统性灾难。

- 案例：

  - 在挑战者号和哥伦比亚号事件中，小问题（O 形密封圈失效或泡沫脱落）通过高度耦合的系统放大为灾难。

#### **2.2 文化与惯例的影响**

- 组织的文化和行为惯例可能在无意中助长失败：
  - 偏差的正常化（Normalization of Deviance）：
    - 组织逐渐接受偏差，将其视为“正常”，降低了对潜在风险的警觉。
  - 目标压力与风险妥协：
    - 任务时间表和成本目标常常优先于安全考虑。

#### **2.3 应对极限的策略**

- 作者提出了一些应对极限挑战的策略：
  - 增强**冗余设计（Redundancy）**：为系统引入更多缓冲和容错机制。
    Establish <span style = "color : red">redundancy</span> mechanisms and introduce more buffering and fault-tolerance mechanisms into the system.
  - 提高**组织灵活性（Organizational Flexibility）**：快速适应变化。
    Improve **Organizational Flexibility**: Adapt to changes quickly.
  - 强化**文化敏感性（Cultural Sensitivity）**：促使员工在面临潜在风险时勇于表达。
    Strengthening Cultural Sensitivity: Encourage employees to speak up when faced with potential risks.

------

### **3. 对“技术失败”主题的贡献**

#### **3.1 技术失败的组织视角**

- 作者强调技术失败并非纯粹的技术问题，而是组织文化、结构和流程的综合产物。
- 系统性失误（Systemic Failure）：
  - 挑战者号和哥伦比亚号的灾难表明，技术和组织因素交织在一起，导致不可预测的后果。

#### **3.2 高风险技术的适应性管理**

- 作者建议，高风险组织需要超越传统管理思维，采用动态适应性策略应对极端环境。

#### **3.3 风险管理中的文化与学习**

- 鼓励组织反思和学习失败案例，通过建立开放的文化避免偏差的正常化。

------

### **4. 技术失败的关键名词中英对照**

| **中文**     | **英文**                     |
| ------------ | ---------------------------- |
| 极限管理     | Organizing at the Limits     |
| 超越极限     | Organizing Beyond the Limits |
| 偏差的正常化 | Normalization of Deviance    |
| 紧密耦合     | Tight Coupling               |
| 复杂性       | Complexity                   |
| 冗余设计     | Redundancy Design            |
| 组织灵活性   | Organizational Flexibility   |
| 系统性失误   | Systemic Failure             |

------

### **5. 应用与启示**

#### **5.1 高风险领域的实践应用**

- 航天领域：在任务设计中引入更多冗余和风险评估机制。
- 医疗与核能：加强对复杂系统的监控和风险预测能力。

#### **5.2 对现代技术的启示**

- 在人工智能和数据科学等新兴领域，高复杂性和紧密耦合性也可能导致系统性失败。
- 建议通过实时监测和动态调整机制，降低技术应用中的潜在风险。

#### **5.3 建立学习型文化**

- 强调从失败中学习的重要性，避免重复犯同样的错误。
- 通过案例分析和跨学科协作，推动组织变得更加敏捷和灵活。

------

### **总结**

Farjoun 和 Starbuck 的文章从理论与实践的角度探讨了组织如何在极限条件下运作及适应变化。他们通过对挑战者号和哥伦比亚号灾难的分析，提出了复杂系统中的失败往往是技术与文化因素交织的结果。文章对技术失败主题的理解具有重要意义，尤其是在高风险技术领域管理和文化转型方面提供了宝贵的见解。





# 8. P. Banerjee & J. Mahoney (2007) review of Starbuck & Farjoun's *Organization at the Limit: Lessons from the Columbia Disaster*,

### **文献标题：**

**Review of \*Organization at the Limit: Lessons from the Columbia Disaster\***
 **作者：** P. Banerjee & J. Mahoney
 **期刊：** *Academy of Management Perspectives*
 **出版时间：** 2007年
 **卷号与页码：** 第21卷第4期，第87-90页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/225251966)

------

### **核心内容概述**

P. Banerjee 和 J. Mahoney 的书评对 W. Starbuck 和 M. Farjoun 编辑的著作 *Organization at the Limit: Lessons from the Columbia Disaster* 进行了深入评价。该书通过分析哥伦比亚号航天飞机灾难，探讨了高风险技术系统中的组织行为及其导致失败的机制。评论聚焦于该书的主要贡献、理论框架和实际意义，同时也指出了一些局限性。

------

### **1. 书中核心内容**

#### **1.1 组织与高风险技术系统**

- 本书以哥伦比亚号灾难为核心案例，揭示了组织文化、流程和决策如何影响高风险技术系统的运作。
- 该书认为技术失败是复杂系统与组织行为相互作用的结果。

#### **1.2 核心问题与分析框架**

- 紧密耦合性（Tight Coupling）：
  - 系统中各部分高度依赖，任何局部问题都会迅速影响整个系统。
- 复杂性（Complexity）：
  - 技术系统越复杂，预测和管理潜在风险的难度越大。
- 组织文化的影响（Organizational Culture’s Impact）：
  - NASA 的文化偏差，包括对时间表和预算的优先关注，掩盖了潜在风险。
- 沟通障碍（Communication Barriers）：
  - 技术团队的警告未能传递到决策层。

#### **1.3 对哥伦比亚号灾难的洞察**

- 泡沫脱落问题（Foam Shedding）：
  - 航天飞机发射时隔热泡沫脱落，撞击机翼导致热保护系统失效。
- 决策中的偏差（Decision Biases）：
  - 偏差的正常化（Normalization of Deviance）使得类似问题在历次发射中被忽视。

------

### **2. Banerjee 和 Mahoney 的评价**

#### **2.1 对本书的赞扬**

1. **跨学科视角的运用：**
   - 本书结合了管理学、社会学和工程学的理论框架，提供了对高风险技术系统失败的全面分析。
   - 强调了组织文化和结构对技术系统的深远影响。
2. **实践意义：**
   - 通过案例研究和理论分析，本书为高风险组织（如航空、医疗、核能）提供了改进管理流程的具体建议。
   - 强调了在复杂系统中培养开放文化和增强沟通的重要性。
3. **决策与文化的结合分析：**
   - 本书深入探讨了偏差正常化如何成为组织决策中的系统性问题，这一观点具有深远意义。

#### **2.2 对本书的批评**

1. **缺乏量化分析：**
   - 评论指出，本书的大部分分析基于定性研究，缺乏数据支持的量化验证。
   - 建议未来的研究结合定量方法，以增强结论的普适性。
2. **案例的单一性：**
   - 尽管哥伦比亚号灾难是一个具有代表性的案例，但依赖单一案例可能限制理论的推广性。
   - 建议将这一框架应用于其他领域的失败案例进行比较研究。
3. **解决方案的具体性不足：**
   - 本书提出了一些普遍性的建议，但在如何实际实施这些改进方面缺乏具体指导。

------

### **3. 技术失败的关键名词中英对照**

| **中文**     | **英文**                  |
| ------------ | ------------------------- |
| 紧密耦合性   | Tight Coupling            |
| 复杂性       | Complexity                |
| 偏差的正常化 | Normalization of Deviance |
| 组织文化     | Organizational Culture    |
| 沟通障碍     | Communication Barriers    |
| 决策偏差     | Decision Biases           |
| 风险评估     | Risk Assessment           |
| 冗余设计     | Redundancy Design         |

------

### **4. 本书对“技术失败”主题的贡献**

#### **4.1 复杂系统中的文化与行为**

- 本书通过对哥伦比亚号灾难的分析，提供了理解复杂技术系统失败的新方法：
  - 技术故障和组织行为的相互作用是失败的关键。
  - 偏差的正常化和文化惯例可能在无意中助长了风险。

#### **4.2 提供跨学科管理框架**

- 本书为高风险技术组织提供了改进流程和文化的框架，强调了以下几点：
  - 加强技术团队与管理层之间的沟通。
  - 打破“沉默文化”，鼓励对潜在风险的公开讨论。
  - 在高风险系统中建立冗余机制和动态反馈流程。

#### **4.3 对未来研究的启示**

- Banerjee 和 Mahoney 指出，本书的分析框架可以扩展到其他高风险领域，例如：
  - 金融系统的崩溃。
    The collapse of the financial system.
  - 医疗技术中的故障。
    Failures in medical technology.
  - 核能或工业事故。
    Nuclear or industrial accidents.

------

### **总结**

P. Banerjee 和 J. Mahoney 高度评价了 *Organization at the Limit: Lessons from the Columbia Disaster*，认为其在高风险技术系统失败的组织文化和行为分析中具有深远意义。尽管本书在量化分析和实践建议的具体性方面存在不足，但其跨学科视角和理论框架为理解技术失败的社会文化根源提供了宝贵的洞察。这篇书评强调了本书在“技术失败”主题中的重要地位，并为高风险组织改进流程和文化提供了参考方向。





# 9. M. Neufeld's 2010 book-review of McDonald & Hansen's *Truth, Lies, and O-Rings: Inside the Space Shuttle Challenger*Disaster[没新东西]

### **文献标题：**

**Review of \*Truth, Lies, and O-Rings: Inside the Space Shuttle Challenger Disaster\***
 **作者：** Michael Neufeld
 **期刊：** *Isis*
 **出版时间：** 2010年
 **卷号与页码：** 第101卷第2期，第452-453页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/650722620)

------

### **核心内容概述**

Michael Neufeld 在这篇评论中评价了 Allan McDonald 和 James Hansen 的著作 *Truth, Lies, and O-Rings: Inside the Space Shuttle Challenger Disaster*。这本书以 McDonald 的亲身经历为基础，详细叙述了挑战者号灾难的技术和组织背景。Neufeld 对该书的详尽分析和个人化叙述方式表示肯定，同时指出了一些局限性。

------

### **1. 书中核心内容**

#### **1.1 技术问题**

- O 形密封圈（O-Rings）的失效：
  - McDonald 详细描述了固体火箭推进器的 O 形密封圈如何在低温下失去弹性，从而导致燃料泄漏和航天飞机解体。
  - 书中补充了大量技术细节，揭示了这一关键问题是如何被反复忽视的。

#### **1.2 组织与文化问题**

- NASA 的决策失败：
  - McDonald 揭示了 NASA 在灾难前的组织文化问题，包括忽视技术警告、优先考虑时间表以及外部政治压力。
- 管理层的责任：
  - 作者特别强调了 NASA 高层对工程团队意见的漠视，指出这是灾难的主要诱因之一。

#### **1.3 个人视角**

- McDonald 提供了第一人称的视角，详细记录了他在灾难前夕反对发射的经历。
  McDonald provides a first-person account, detailing his experiences opposing the launch on the eve of the disaster.
- 伦理与责任：
  - 书中反思了工程师在高风险技术项目中的伦理责任。

------

### **2. Neufeld 的评价**

#### **2.1 对本书的赞扬**

1. **技术与组织分析的深度：**
   - Neufeld 对书中技术问题的详尽分析和对 NASA 组织文化的批判表示肯定。
   - 认为这本书为理解挑战者号灾难的复杂性提供了新的视角。
2. **第一人称叙述的力量：**
   - McDonald 的亲身经历和情感投入使得本书不仅仅是技术报告，还具有极强的故事性和说服力。
3. **道德反思的重要性：**
   - 本书揭示了工程师如何在高风险技术决策中平衡科学伦理与组织压力。

#### **2.2 对本书的批评**

1. **叙述可能的偏颇：**
   - 由于本书以 McDonald 的视角为主，可能忽略了 NASA 其他成员的观点。
   - Neufeld 认为这种主观性在一定程度上影响了全局分析的平衡性。
2. **外部压力的探讨不足：**
   - 书中更多关注 NASA 内部的文化和管理问题，而对外部政治和经济压力的分析不够深入。

------

### **3. 技术失败的关键名词中英对照**

| **中文**       | **英文**                     |
| -------------- | ---------------------------- |
| 固体火箭推进器 | Solid Rocket Booster (SRB)   |
| O 形密封圈     | O-Rings                      |
| 偏差的正常化   | Normalization of Deviance    |
| 组织文化       | Organizational Culture       |
| 风险评估       | Risk Assessment              |
| 工程伦理       | Engineering Ethics           |
| 管理决策失败   | Managerial Decision Failures |

------

### **4. 本书对“技术失败”主题的贡献**

#### **4.1 技术细节的深入分析**

- 本书提供了关于 O 形密封圈失效的详尽技术细节，对理解灾难的技术原因至关重要。

#### **4.2 组织行为与文化的深度反思**

- 本书揭示了 NASA 文化中对时间表的过度关注如何导致对技术警告的忽视。

#### **4.3 工程师的道德困境**

- McDonald 的叙述突出了技术人员在面对高风险决策时的责任感和道德困境。

#### **4.4 提供历史视角**

- Neufeld 认为，本书通过第一手资料补充了挑战者号灾难的历史记录，使其成为理解这一事件的重要资源。

------

### **总结**

Michael Neufeld 的书评对 *Truth, Lies, and O-Rings* 表示高度认可，称其为研究挑战者号灾难的必读之作。尽管存在叙述偏颇和视角局限等问题，McDonald 的书通过详细的技术分析和个人经历，为高风险技术项目中的伦理、文化和决策问题提供了深刻反思。这篇书评进一步强调了本书在“技术失败”主题中的重要性及其跨学科的研究价值。





# 10. Charles Perrow在 Microsoft Research 的演讲 [!!!]

### **视频标题：**

**Charles Perrow Lecture at Microsoft Research**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=iTnjXCsHM4c) (1h23m32s)

------

### **核心内容概述**

Charles Perrow 在这场演讲中介绍了他的经典理论——**正常事故理论（Normal Accident Theory, NAT）**。他解释了高风险技术系统中的复杂性和紧密耦合如何导致灾难，指出在某些系统中，事故是不可避免的。他结合多个案例（包括核电站、航天灾难、金融危机等），探讨了这些理论在现代技术和组织中的应用。

------

### **1. 正常事故理论（Normal Accident Theory, NAT）**

#### **1.1 基本假设**

- **定义：**
   正常事故是指在复杂且高度耦合的技术系统中，事故是不可避免的，即使在最严格的管理下也无法完全避免。
- 关键因素：
  - 复杂性（Complexity）：
    - 系统的多层次和多组件增加了预测和控制难度。
    - 例子：核电站的监控系统由多层交互的子系统组成。
  - 紧密耦合（Tight Coupling）：
    - 系统中组件彼此高度依赖，局部问题容易迅速扩散为全局危机。
    - 例子：航天器中的部件故障迅速影响整个任务。

#### **1.2 系统的分类**

- Perrow 将系统分为四种类型：
  1. **简单-松散耦合（Simple-Loosely Coupled）：** 容易管理（如农场）。
  2. **简单-紧密耦合（Simple-Tightly Coupled）：** 容错性较低（如流水线）。
     Low fault tolerance (such as pipeline)
  3. **复杂-松散耦合（Complex-Loosely Coupled）：** 复杂但灵活（如大学）。
  4. **复杂-紧密耦合（Complex-Tightly Coupled）：** 最危险的类型，易引发灾难（如核电站、航天器）。

#### **1.3 案例分析**

- 三哩岛核事故（Three Mile Island）：
  - 核电站的复杂性和紧密耦合导致操作人员难以及时识别和处理问题，最终引发事故。
- 挑战者号航天飞机灾难：
  - Perrow 认为，复杂的航天任务中任何一个小问题（如 O 形密封圈失效）都可能因紧密耦合而升级为灾难。

------

### **2. 事故的社会文化维度**

#### **2.1 组织文化对事故的影响**

- 文化偏差（Cultural Deviance）：
  - 组织在不断容忍风险和异常时，容易将其视为“常态”。
  - 例子：NASA 在挑战者号任务中的风险管理失误。
- 信息屏蔽（Information Silos）：
  - 大型组织中的部门隔离可能导致关键信息未能及时传递。

#### **2.2 社会系统与技术系统的交互**

- 高风险技术系统常与社会系统交互，放大了事故的影响。
- 例子：
  - 金融系统中的紧密耦合性导致2008年全球金融危机。

------

### **3. 减少高风险系统中事故的方法**

#### **3.1 降低复杂性**

- 减少不必要的复杂性可以提高系统的可管理性。
- 例子：核电站通过优化设计减少监控和操作的复杂程度。

#### **3.2 松散耦合**

- 引入冗余和缓冲机制，降低紧密耦合的风险。
- 例子：在航天系统中增加冗余设计，减少单点故障的可能性。

#### **3.3 提高透明度**

- 改善信息共享机制，确保每个部门和层级的沟通顺畅。

#### **3.4 文化变革**

- 组织需要建立一种“安全第一”的文化，鼓励员工提出风险并及时应对。

------

### **4. 技术失败的关键名词中英对照**

| **中文**     | **英文**                     |
| ------------ | ---------------------------- |
| 正常事故理论 | Normal Accident Theory (NAT) |
| 复杂性       | Complexity                   |
| 紧密耦合     | Tight Coupling               |
| 松散耦合     | Loose Coupling               |
| 文化偏差     | Cultural Deviance            |
| 信息屏蔽     | Information Silos            |
| 冗余设计     | Redundancy Design            |

------

### **5. Perrow 的演讲对“技术失败”主题的贡献**

#### **5.1 理论深度**

- 正常事故理论为理解高风险技术系统的失败提供了强有力的理论框架，特别适用于核电、航天和金融领域。

#### **5.2 组织行为的重要性**

- Perrow 强调，组织文化和行为对事故的发生和防止起着关键作用。

#### **5.3 提供实践建议**

- 他提出的减少复杂性、松散耦合和文化变革的策略为高风险组织提供了具体的改进方向。
  His strategies of reducing complexity, loose coupling, and cultural change provide specific directions for improvement for high-risk organizations.

#### **5.4 提示伦理责任**

- Perrow 提醒技术系统设计者和组织决策者，要对高风险系统中的潜在事故承担更大的伦理责任。

------

### **总结**

Charles Perrow 的演讲通过深入分析“正常事故理论”，揭示了高风险技术系统失败的内在机制。他的观点不仅具有理论意义，还为改善高风险组织的管理提供了实践指导。这场演讲为“技术失败”主题提供了重要的跨学科洞见，强调了复杂性、耦合性和文化因素在技术灾难中的核心作用。



# 11. D. L . Simms (1986) review of *Normal Accidents [!!!]*

### **Perrow 提出的改进高风险组织的策略**

Perrow's Strategies for Improving High-Risk Organizations

Charles Perrow 在其**正常事故理论（Normal Accident Theory, NAT）**中，不仅揭示了高风险技术系统中事故的不可避免性，还提出了具体策略来减少这些系统中事故的概率。以下是他提出的核心策略：

------

### **1. 减少复杂性（Reducing Complexity）**

#### **核心观点：**

- **复杂性（Complexity）**是高风险系统中事故发生的主要诱因之一。随着系统中组件和交互的增加，预测和控制问题变得极为困难。
- 减少复杂性可以提高系统的透明度和可控性，从而降低事故的发生概率。

#### **实施方法：**

1. **简化设计：**
   - 避免不必要的技术功能和交互，采用更简单、更直观的系统设计。
     Simplify the design, avoid unnecessary technical functions and interactions, and adopt a simpler and more intuitive system design.
   - **案例：** 核电站的设计优化减少了不必要的监控和反馈回路。
2. **模块化系统：**
   - 将复杂系统划分为独立模块，以便隔离故障，防止问题扩散。
     Divide complex systems into independent modules to isolate faults and prevent problems from spreading.
   - **案例：** 软件开发中的模块化设计有助于在单一模块失败时保护其他模块。
3. **清晰的操作流程：**
   - 制定简洁、易懂的操作规程，避免复杂性引发的人为错误。
     Develop concise and easy-to-understand operating procedures to avoid human errors caused by complexity.

------

### **2. 引入松散耦合（Loose Coupling）**

#### **核心观点：**

- **紧密耦合（Tight Coupling）**会使系统中的局部故障迅速升级为全局灾难，而**松散耦合（Loose Coupling）**则通过引入缓冲机制和冗余设计，降低了这种扩散风险。

#### **实施方法：**

1. **缓冲机制：**Buffering mechanism:
   - 为关键系统组件引入时间或资源缓冲，以提供故障修复的空间。
     Introduce time or resource buffers for critical system components to provide room for failure recovery.
   - **案例：** 在金融交易系统中设置交易延迟缓冲，防止算法崩溃扩散。
2. **冗余设计（Redundancy Design）：**
   - 增加系统中的备用组件或备用路径，使系统在局部失效时仍能运行。
     Add backup components or backup paths to the system so that the system can still operate in the event of a partial failure.
   - **案例：** 航空系统中采用双引擎或多路径通信机制。
3. **解耦关键组件：**Decoupling key components
   - 避免关键组件之间的直接相互依赖，通过独立性设计减少连锁反应的风险。
     Reduce the risk of chain reactions through independent design.
   - **案例：** 供应链管理中引入多供应商机制，减少单点依赖。

------

### **3. 促进文化变革（Cultural Change）**

#### **核心观点：**

- 组织文化在风险管理中起着至关重要的作用。通过**文化变革（Cultural Change）**，可以改善风险意识、促进信息共享、消除侥幸心理和沉默文化。

#### **实施方法：**

1. **建立“安全优先”文化：**
   - 将安全置于任务进度和成本控制之上，确保员工在决策中始终优先考虑风险。
   - **案例：** 核电站和航空行业采用“零容忍事故”的文化。
2. **鼓励反馈和异议：**
   - 为员工创造一个可以自由表达安全问题的环境，避免因惧怕报复而隐瞒潜在风险。
   - **案例：** NASA 在挑战者号灾难后引入了匿名报告机制。
3. **提升风险意识：**
   - 通过培训和案例分析，让员工了解复杂系统中的风险来源和处理方法。
   - **案例：** 医疗行业中的安全文化培训，强调如何处理紧急情况。
4. **强化领导层责任：**
   **STRENGTHENING LEADERSHIP RESPONSIBILITY:**
   - 领导层需以身作则，支持安全决策，并积极响应下属提出的安全警告。
   - **案例：** 制定明确的问责制度，确保决策透明。

------

### **Perrow 策略的意义**

#### **1. 综合性**

- 这三种策略相辅相成，分别从技术、组织结构和文化角度着手，全面改善高风险系统的安全性。

#### **2. 适用性**

- 这些策略不仅适用于核电、航空和航天等传统高风险行业，也适用于现代技术领域（如人工智能、金融科技和大数据系统）。

#### **3. 强调长期改进**

- 通过逐步减少复杂性、降低耦合程度和改进文化，高风险组织可以建立更具弹性和适应性的系统。

------

### **总结**

Charles Perrow 的策略为高风险组织提供了清晰的方向，即通过**降低复杂性**、**引入松散耦合**和**促进文化变革**，减少灾难发生的可能性。这些措施不仅有助于技术系统的优化，还能提升组织对潜在风险的敏感性，为现代高风险行业的管理提供了宝贵的实践指导。





# 12. J. Ravetz (1985) review of *Normal Accidents,* in *Futures*

### **文献标题：**

**Review of \*Normal Accidents\* by Charles Perrow**
 **作者：** J. Ravetz
 **期刊：** *Futures*
 **出版时间：** 1985年
 **卷号与页码：** 第17卷第3期，第287-288页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/4662372945)

------

### **核心内容概述**

J. Ravetz 在这篇评论中对 Charles Perrow 的经典著作 *Normal Accidents: Living with High-Risk Technologies* 进行了评价。Ravetz 高度认可 Perrow 提出的**正常事故理论（Normal Accident Theory, NAT）**，并探讨了该理论在社会技术系统中的深远影响。他认为 Perrow 的研究为理解复杂技术系统的失败提供了重要的理论基础，同时也对高风险技术的管理和伦理责任提出了挑战。

------

### **1. Perrow 的核心观点**

#### **1.1 正常事故理论（Normal Accident Theory, NAT）**

- 定义：
  - 在复杂且紧密耦合的技术系统中，事故并非异常，而是不可避免的。
    In complex and tightly coupled technical systems, accidents are not anomalies but inevitable.
  - 即使在最严格的安全管理下，技术系统的复杂性和不可预测性都会导致失败。
    Even with the most stringent security management, the complexity and unpredictability of technology systems can lead to failures.

#### **1.2 系统的复杂性和紧密耦合**

- 复杂性（Complexity）：
  - 系统内的多组件和多路径交互增加了预测和控制的难度。
- 紧密耦合（Tight Coupling）：
  - 系统中组件的高度依赖性使局部问题容易升级为系统性灾难。

#### **1.3 高风险技术的特征**

- 高风险技术（如核电站、航天器和化工厂）是“正常事故”理论的典型应用场景：
  - 这些技术系统因其复杂性和高耦合性，始终处于潜在的失效风险中。

------

### **2. Ravetz 对 Perrow 的评价**

#### **2.1 高度认可 Perrow 的理论贡献**

- Ravetz 认为 Perrow 的研究为理解复杂技术系统的内在脆弱性提供了革命性框架。
- 特别是“正常事故”这一概念，为分析高风险技术的失败机制提供了深刻洞见。

#### **2.2 对技术乐观主义的质疑**

- Perrow 的理论挑战了传统的技术乐观主义：
  - 技术进步并非总是带来安全和效率，反而可能增加不可预测的风险。
  - Ravetz 认为，这一观点对技术治理具有重要的警示意义。

#### **2.3 理论的适用性**

- Ravetz 称赞 Perrow 的理论不仅适用于传统高风险技术，也可以扩展到金融、交通等其他复杂系统。

------

### **3. Ravetz 提出的批评与思考**

#### **3.1 方法论的局限**

- Ravetz 指出，Perrow 的分析更多基于案例研究，缺乏量化数据的支持。
- 建议未来研究结合定量方法，进一步验证和拓展“正常事故”理论。

#### **3.2 应对策略的不足**

- 虽然 Perrow 强调了复杂系统中的事故不可避免性，但他未能提供足够的具体解决方案。
- Ravetz 呼吁在技术设计和管理中引入更多预警和缓解机制。

#### **3.3 对社会伦理的关注**

- Ravetz 认为，Perrow 对技术失败的社会后果和伦理责任的讨论仍显不足，未来研究应更关注这些问题。

------

### **4. 技术失败的关键名词中英对照**

| **中文**     | **英文**                       |
| ------------ | ------------------------------ |
| 正常事故理论 | Normal Accident Theory (NAT)   |
| 复杂性       | Complexity                     |
| 紧密耦合     | Tight Coupling                 |
| 高风险技术   | High-Risk Technologies         |
| 技术乐观主义 | Technological Optimism         |
| 社会伦理     | Social Ethics                  |
| 事故缓解机制 | Accident Mitigation Mechanisms |

------

### **5. Perrow 理论对“技术失败”主题的贡献**

#### **5.1 理解技术失败的内在机制**

- 正常事故理论将技术失败从偶然事件转变为结构性问题，强调复杂系统中的内在脆弱性。

#### **5.2 启发跨领域研究**

- Ravetz 认为，Perrow 的理论适用于核能、航天、金融等多个领域，为复杂系统的风险管理提供了理论支持。

#### **5.3 提升对技术治理的关注**

- Perrow 的观点提醒政策制定者和技术开发者，在设计和管理复杂系统时，需要更加关注不可预测性和系统性风险。

------

### **总结**

J. Ravetz 对 *Normal Accidents* 的评价高度肯定了 Charles Perrow 的理论贡献，同时提出了一些方法论和实践层面的改进建议。Ravetz 认为，正常事故理论在“技术失败”主题中具有重要的理论价值和实践意义，为高风险技术的管理、设计和社会责任提供了深刻的启示。这篇评论进一步强化了 Perrow 理论在理解复杂技术系统失败中的核心地位。



# 13. Brief video where James Reason explains his "Swiss Cheese Model" [奶酪模型]

### **视频标题：**

**James Reason Explains the "Swiss Cheese Model"**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=KND5py-z8yI) (4m08s)

------

### **核心内容概述**

在这段视频中，James Reason 简明扼要地介绍了他的**“瑞士奶酪模型（Swiss Cheese Model）”**，用以解释技术事故的发生机制。他通过直观的类比说明，高风险系统中的事故往往是多个层次防御机制同时失效的结果，而非单一原因引发的事件。

------

### **1. 瑞士奶酪模型的核心观点**

#### **1.1 模型的基本概念**

- 瑞士奶酪类比：
  - 每一层奶酪代表一个**防御层（Defense Layer）**，如安全协议、操作规程或技术设计。
  - **洞（Holes）**代表这些防御层的弱点或漏洞，可能源于设计缺陷、人为错误或环境因素。
- 关键机制：
  - 当所有防御层的洞意外对齐时，一个事故路径形成，导致灾难。
    When the holes in all the defense layers accidentally aligned, an accident path was formed, leading to disaster.

#### **1.2 事故的系统性**

- 事故不是单一原因的结果，而是**系统性失效（Systemic Failure）**的产物。
- 各种因素可能相互作用，贯穿防御层，形成事故链条。

#### **1.3 动态与潜在漏洞**

- 动态漏洞（Active Failures）：
  - 即时性问题，例如操作人员的失误。
- 潜在漏洞（Latent Failures）：
  - 隐藏的系统问题，例如设计缺陷、组织文化问题或长期的管理疏忽。

------

### **2. 瑞士奶酪模型的应用领域**

#### **2.1 高风险技术系统**

- 航空领域：
  - 飞行员操作失误与技术设备故障同时发生可能导致飞机事故。
- 医疗领域：
  - 药物错误和诊断设备问题可能同时作用，导致病人伤害。
- 核能领域：
  - 系统设计缺陷与操作失误可能导致核泄漏事故。

#### **2.2 组织管理**

- 强调组织文化和沟通在事故预防中的重要性。
- 鼓励在操作流程中建立多层防御机制，避免单点故障扩展为全系统问题。

------

### **3. 瑞士奶酪模型的关键名词中英对照**

| **中文**     | **英文**           |
| ------------ | ------------------ |
| 瑞士奶酪模型 | Swiss Cheese Model |
| 防御层       | Defense Layer      |
| 洞           | Holes              |
| 动态漏洞     | Active Failures    |
| 潜在漏洞     | Latent Failures    |
| 系统性失效   | Systemic Failure   |
| 事故链条     | Accident Pathway   |

------

### **4. 视频的核心观点与启示**

#### **4.1 多层防御的重要性**

- 模型启示：
  - 设计高风险技术系统时，应增加防御层，减少漏洞对齐的概率。
  - 冗余设计和双重检查是有效的防御机制。

#### **4.2 潜在漏洞的监控**

- 潜在漏洞往往比动态漏洞更危险，因为它们难以察觉且可能长期存在。
- 组织需要通过定期审查和监控来识别和修复潜在问题。

#### **4.3 文化与沟通**

- 开放的组织文化和有效的沟通可以在事故链条形成之前识别问题。
- 鼓励员工报告潜在风险，而不担心惩罚，是预防事故的重要手段。

------

### **5. 与技术失败主题的关联**

#### **5.1 系统性视角**

- 瑞士奶酪模型为理解技术失败提供了一个**系统性视角**，强调事故是多层次问题共同作用的结果。

#### **5.2 风险管理与预防**

- 模型为风险管理提供了具体指导：
  - 建立多层次的防御机制。
  - 重视潜在漏洞和动态漏洞的结合。

#### **5.3 借鉴意义**

- 瑞士奶酪模型可广泛应用于航空、医疗、核能、金融等多个高风险领域，为预防技术失败提供了重要工具。

------

### **总结**

James Reason 的“瑞士奶酪模型”以简单的类比揭示了技术事故的复杂性，强调事故是多层防御失效的系统性产物。通过增加防御层、监控潜在漏洞和促进沟通，组织可以减少事故发生的可能性。这一模型为“技术失败”主题提供了宝贵的理论框架和实践指导。



# 14. Simplified Explanation of James Reason's Swiss Cheese Model [进一步拓展]

### **视频标题：**

**Simplified Explanation of James Reason's Swiss Cheese Model**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=twsA3z3xFVE)

------

### **核心内容概述**

这段视频用更简单、更直观的方式解释了**James Reason 的瑞士奶酪模型（Swiss Cheese Model）**，展示了如何通过类比理解事故是多个防御层同时失效的结果。视频通过动画和清晰的语言，将复杂理论转化为易于理解的概念，适合更广泛的观众。

------

### **1. 瑞士奶酪模型的简化解释**

#### **1.1 防御层与漏洞**

- 每层瑞士奶酪代表一个**防御层（Defense Layer）**，如安全规程、技术设计或管理制度。
- 奶酪上的**洞（Holes）**表示漏洞：
  - 每个防御层可能因缺陷或失误无法完全阻挡风险。

#### **1.2 漏洞对齐与事故发生**

- 当多个防御层的洞意外对齐时，就会形成事故路径（Accident Pathway）。
- 这表明事故往往不是单一原因，而是多个因素共同作用的结果。

------

### **2. 事故类型的区分**

#### **2.1 动态漏洞（Active Failures）**

- 即时发生的问题，例如操作失误或设备故障。
- **例子：** 飞行员在操作时按错按钮。

#### **2.2 潜在漏洞（Latent Failures）**

- 隐藏的系统性问题，例如设计缺陷或长期疏忽。
- **例子：** 飞机设计中未被发现的安全隐患。

------

### **3. 防止漏洞对齐的方法**

#### **3.1 增加防御层**

- 建立更多层次的防御机制，减少所有漏洞对齐的概率。
- **例子：** 在医院中增加多重检查程序以防止药物错误。

#### **3.2 修补漏洞**

- 通过改进设计和培训减少潜在漏洞的数量和规模。
- **例子：** 核电站定期审查安全系统，修复潜在问题。

#### **3.3 提升透明度**

- 鼓励报告潜在漏洞和动态失误，建立开放的组织文化。

------

### **4. 瑞士奶酪模型的关键名词中英对照**

| **中文**     | **英文**           |
| ------------ | ------------------ |
| 瑞士奶酪模型 | Swiss Cheese Model |
| 防御层       | Defense Layer      |
| 洞           | Holes              |
| 动态漏洞     | Active Failures    |
| 潜在漏洞     | Latent Failures    |
| 事故路径     | Accident Pathway   |

------

### **5. 视频的核心亮点**

#### **5.1 简单直观**

- 使用动画演示，让观众能够快速抓住核心概念。

#### **5.2 强调系统性**

- 通过动态与潜在漏洞的结合，清晰展示了事故的系统性原因。

#### **5.3 提供具体措施**

- 视频强调了如何通过增加防御层和修补漏洞来降低事故风险。

------

### **总结**

这段视频以简洁清晰的方式解释了瑞士奶酪模型，将复杂的理论转化为直观的视觉演示。通过展示动态漏洞和潜在漏洞的结合，观众能够快速理解事故的系统性成因，并学到减少事故风险的实际方法。模型的核心理念对于高风险行业的风险管理具有重要意义。



# 15. D. Clark (2011) review of Reason's *The Human Contribution*

### **文献标题：**

**Review of \*The Human Contribution: Unsafe Acts, Accidents, and Heroic Recoveries\***
 **作者：** D. Clark
 **期刊：** *Journal of Risk Research*
 **出版时间：** 2011年
 **卷号与页码：** 第14卷第1期，第144-145页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/4644993787)

------

### **核心内容概述**

D. Clark 在这篇评论中对 James Reason 的著作 *The Human Contribution: Unsafe Acts, Accidents, and Heroic Recoveries* 进行了评价。这本书探讨了人类在技术系统中既作为事故原因又作为事故恢复关键角色的双重身份。Clark 高度评价了 Reason 对**人类行为在高风险环境中的作用**的深刻洞察，并赞扬了作者将理论与案例相结合的叙述方式。

------

### **1. 书中核心观点**

#### **1.1 人类行为的双重角色**

- Reason 将人类视为

  双面角色（Dual Role）：

  - 事故的促成者（Contributors to Accidents）：
    - 动态失误（Active Failures）：如操作失误或决策错误。
    - 潜在失误（Latent Conditions）：如设计缺陷或管理疏忽。
  - 事故的英雄恢复者（Heroic Recoveries）：
    - 人类的灵活性和创造性在危机中能够有效补救技术系统的失误。

#### **1.2 系统性失败的框架**

- 通过

  瑞士奶酪模型（Swiss Cheese Model）

  ，Reason 阐明了事故的系统性成因：

  - 动态漏洞和潜在漏洞结合，贯穿系统的多层防御，导致事故发生。

#### **1.3 英雄式恢复的案例**

The Case for a Heroic Recovery

- Reason 提供了多个真实案例，说明人类在高风险环境中如何通过快速反应和创造性思维挽救局势：
  - **航空业：** 飞行员在仪器失灵时的临场应对。
  - **医疗领域：** 医务人员在紧急情况下的诊断和救治。

------

### **2. Clark 对 Reason 的评价**

#### **2.1 对书中优点的认可**

1. **案例研究的丰富性：**
   - Clark 称赞 Reason 通过真实案例生动地展示了理论，并使抽象概念更易理解。
   - 案例的选择涵盖了多个领域，如航空、医疗和核能，具有广泛的适用性。
2. **理论与实践的结合：**
   - Reason 的叙述将系统性风险理论与人类行为的实际影响相结合，为高风险系统管理提供了实用建议。
3. **正视人类的积极作用：**
   - Clark 特别认可 Reason 强调人类在复杂系统中的“英雄角色”，这一观点为风险管理带来了平衡的视角。

#### **2.2 对书中不足的批评**

1. **对“英雄恢复”的过度强调：**
   - Clark 认为，Reason 对人类创造性和灵活性的强调可能会掩盖系统设计中减少失误的重要性。
   - 过度依赖“英雄恢复”可能会让组织忽视预防性设计。
2. **学术分析的局限性：**
   - 虽然案例丰富，但 Reason 在某些理论分析上缺乏深度。
   - Clark 建议可以结合更多的量化数据，支持理论框架的普适性。

------

### **3. 技术失败的关键名词中英对照**

| **中文**         | **英文**                  |
| ---------------- | ------------------------- |
| 双面角色         | Dual Role                 |
| 事故的促成者     | Contributors to Accidents |
| 事故的英雄恢复者 | Heroic Recoveries         |
| 动态漏洞         | Active Failures           |
| 潜在漏洞         | Latent Conditions         |
| 瑞士奶酪模型     | Swiss Cheese Model        |
| 系统性失败       | Systemic Failure          |

------

### **4. Reason 的研究对技术失败主题的贡献**

#### **4.1 对人类行为的平衡视角**

- Reason 提出了更全面的框架，将人类行为的缺陷与潜力结合起来。
- 这一视角帮助组织在风险管理中不仅关注技术改进，也考虑人类因素的优化。

#### **4.2 案例驱动的风险管理**

- 通过对多个真实案例的分析，Reason 提供了具体的管理建议，例如：
  - 增加防御层，减少潜在漏洞。
    Add layers of defense and reduce potential vulnerabilities.
  - 加强培训，提升人员在危机中的响应能力。
    Strengthen training to enhance personnel's response capabilities in crises.

#### **4.3 提升对英雄行为的重视**

- Reason 的研究强调，人类行为在高风险系统中的积极作用不应被忽视。
- 鼓励组织在系统设计中为“英雄恢复”留有空间，例如提供更灵活的应急措施。

------

### **总结**

D. Clark 的书评充分肯定了 James Reason 在 *The Human Contribution* 中对人类行为与技术失败关系的深入探讨，同时也指出了一些理论和方法上的不足。Clark 强调，Reason 的研究为理解技术失败提供了平衡视角，将人类视为既是事故促成者，也是关键恢复者。这一观点对风险管理和组织文化的改进具有重要意义，尤其是在高风险技术领域的实践中。



# 16. Review of *The Human Contribution: Unsafe Acts, Accidents, and Heroic Recoveries*

### **文献标题：**

**Review of \*The Human Contribution: Unsafe Acts, Accidents, and Heroic Recoveries\***
 **作者：** M. Young
 **期刊：** *Ergonomics*
 **出版时间：** 2010年
 **卷号与页码：** 第53卷第4期，第586-587页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/6895372705)

------

### **核心内容概述**

M. Young 对 James Reason 的著作 *The Human Contribution: Unsafe Acts, Accidents, and Heroic Recoveries* 进行了深入评论。这本书重点探讨了人类在高风险技术系统中的角色，既是事故的促成者，也是事故的补救者。Young 在评论中高度评价了 Reason 的洞察力、理论框架和对人类行为的细致分析，同时指出了一些不足之处。

------

### **1. 书中核心观点**

#### **1.1 人类的双重角色**

- Reason 提出了人类在技术系统中的双重角色：
  - 事故的促成者（Contributors to Accidents）：
    - 动态失误（Active Failures）：即时发生的错误，如操作错误或决策失误。
    - 潜在失误（Latent Failures）：长期存在的系统性问题，如设计缺陷或管理疏漏。
  - 事故的英雄恢复者（Heroic Recoveries）：
    - 人类的灵活性和适应能力在关键时刻能够挽救技术系统的失败。

#### **1.2 瑞士奶酪模型（Swiss Cheese Model）**

- Reason 再次运用了**瑞士奶酪模型**，解释了如何通过多层防御机制减少事故路径。
- 书中强调了漏洞（动态和潜在）对事故发生的累积影响。

#### **1.3 英雄行为的案例**

- Reason 提供了多个真实案例，展示了人类如何在危机中通过快速反应和创造性思维进行补救：
  - 航空事故中飞行员的应急处理。
  - 医疗系统中医务人员对错误诊断的补救。

------

### **2. Young 的评价**

#### **2.1 对书中优点的肯定**

1. **案例分析的丰富性和多样性：**
   - Young 称赞 Reason 在书中提供了大量案例，涵盖了航空、医疗、核能等领域。
   - 这些案例不仅丰富了理论，也增强了读者对实际应用的理解。
2. **理论框架的清晰性：**
   - 瑞士奶酪模型和双重角色框架让读者能够轻松理解复杂的事故机制。
   - Young 认为，Reason 成功地将抽象理论转化为实际工具。
3. **对人类行为的正面解读：**
   - Young 特别赞赏 Reason 对人类创造性和恢复力的重视，认为这是对传统事故分析中人类角色的有力补充。

#### **2.2 对书中不足的批评**

1. **英雄行为的局限性：**
   - Young 认为，Reason 对英雄行为的强调可能导致组织过度依赖个体能力，而忽视系统性预防措施的重要性。
     Young argues that Reason’s emphasis on heroic acts can lead organizations to over-rely on individual capabilities and ignore the importance of systemic preventive measures.
2. **对潜在失误的深入探讨不足：**
   - 尽管潜在失误被提及，但 Young 指出，书中缺乏对如何系统性识别和解决这些问题的详细策略。
3. **理论扩展的缺乏：**
   - Young 建议，书中对人类行为的分析可以结合更多心理学和认知科学的最新研究，以增加理论的深度和广度。

------

### **3. 技术失败的关键名词中英对照**

| **中文**         | **英文**                  |
| ---------------- | ------------------------- |
| 双重角色         | Dual Role                 |
| 事故的促成者     | Contributors to Accidents |
| 事故的英雄恢复者 | Heroic Recoveries         |
| 动态失误         | Active Failures           |
| 潜在失误         | Latent Failures           |
| 瑞士奶酪模型     | Swiss Cheese Model        |
| 风险管理         | Risk Management           |

------

### **4. Reason 的研究对技术失败主题的贡献**

#### **4.1 强调人类行为的重要性**

- Reason 的研究表明，事故分析不仅应关注技术系统的设计，还需深入研究人类行为对系统安全的影响。

#### **4.2 提供实用工具**

- 瑞士奶酪模型为风险管理提供了一个系统框架，帮助组织理解和减少漏洞对齐的可能性。

#### **4.3 鼓励对人类行为的正面解读**

- Reason 的观点鼓励组织重视人类的灵活性和恢复能力，而非一味强调其不足。

#### **4.4 启发跨学科合作**

- Young 认为，Reason 的研究为工程学、心理学和管理学的跨学科合作提供了理论基础。

------

### **总结**

M. Young 的书评充分肯定了 James Reason 在 *The Human Contribution* 中对人类行为的深刻洞察，同时指出了理论和案例应用中的局限性。Young 认为，这本书通过展示人类在事故中的双重角色，为风险管理提供了新视角，但也需要进一步扩展其理论基础。总的来说，这本书对“技术失败”主题具有重要意义，特别是在高风险系统的管理和文化建设中。



# 17. A. Roland (1986) review of Petroski's *To Engineer is Human*[失败的意义 可以好好看]

### **文献标题：**

**Review of \*To Engineer is Human: The Role of Failure in Successful Design\***
 **作者：** A. Roland
 **期刊：** *Isis*
 **出版时间：** 1986年
 **卷号与页码：** 第77卷第3期，第562-563页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/4637174961)

------

### **核心内容概述**

A. Roland 在这篇评论中对 Henry Petroski 的著作 *To Engineer is Human: The Role of Failure in Successful Design* 表达了高度评价。这本书探讨了**失败在工程设计中的关键作用**，强调失败不仅是工程的不可避免部分，更是推动创新和改进的重要动力。
Emphasize that failure is not only an inevitable part of engineering, but also an important driving force for innovation and improvement.

Roland 认为，Petroski 用引人入胜的叙述和丰富的案例，将复杂的工程概念转化为普通读者可以理解的语言，同时也为工程师和设计师提供了宝贵的反思工具。

------

### **1. 书中核心观点**

#### **1.1 工程失败的意义**

- Petroski 强调，

  失败是成功设计的基础

  - 每一次失败都提供了宝贵的学习机会。
  - 工程师通过分析失败案例改进设计，避免未来的灾难。

#### **1.2 历史案例分析**

- 本书引用了多个著名工程失败案例，如：
  - 泰晤士河大桥倒塌（Thames River Bridge Collapse）：
    - 展示了如何通过分析桥梁设计的失误推动桥梁工程学的发展。
  - 堤坝和水坝的设计失败：
    - 说明了材料科学和施工技术如何从失败中获得进步。

#### **1.3 工程设计的哲学**

- 设计的迭代性：
  - 工程设计从来不是完美的，而是一个试验、失败、学习、再试验的循环过程。
- 不确定性与风险：
  - 工程师必须在面对不完全信息的情况下做出决策，容忍一定的失败是不可避免的。

------

### **2. Roland 对 Petroski 的评价**

#### **2.1 对书中优点的肯定**

1. **叙述的吸引力：**
   - Petroski 使用了通俗易懂的语言和生动的故事，使复杂的工程概念能够被广泛受众理解。
   - Roland 特别称赞了 Petroski 对技术问题的叙述既严谨又具有文学性。
2. **工程哲学的深度：**
   - Petroski 通过失败的视角，挑战了“完美设计”的传统观念，提出工程学是不断试验和优化的过程。
   - Roland 认为这种视角为读者提供了对工程的全新理解。
3. **案例研究的丰富性：**
   - 书中涵盖了历史和现代的工程失败案例，展示了失败如何推动学科进步。

#### **2.2 对书中不足的批评**

1. **学术深度的限制：**
   - Roland 指出，尽管本书适合广泛受众，但对专业工程师来说可能显得过于通俗化。
   - 他建议 Petroski 在未来的作品中增加对技术细节和数学模型的探讨。
2. **对成功案例的分析不足：**
   - 本书更多关注失败，较少分析成功设计中的风险管理。
   - Roland 认为，成功案例的分析可以提供更全面的视角。

------

### **3. 技术失败的关键名词中英对照**

| **中文**   | **英文**            |
| ---------- | ------------------- |
| 工程失败   | Engineering Failure |
| 迭代设计   | Iterative Design    |
| 风险管理   | Risk Management     |
| 材料科学   | Material Science    |
| 桥梁工程学 | Bridge Engineering  |

------

### **4. Petroski 的研究对技术失败主题的贡献**

#### **4.1 强调失败的教育意义**

- Petroski 提出了失败不仅是负面事件，更是工程师学习和改进的重要机会。

#### **4.2 构建反思框架**

- 通过历史案例分析，Petroski 为工程师和设计师提供了如何分析失败的框架：
  - 查明失败的根本原因。
  - 在设计中引入冗余和容错机制。

#### **4.3 普及工程学概念**

- 本书通过通俗化的叙述，使非专业读者也能理解工程设计的复杂性和失败的重要性。

------

### **总结**

A. Roland 的评论充分肯定了 Henry Petroski 在 *To Engineer is Human* 中对工程失败和设计哲学的深刻探讨。Roland 认为，这本书以失败为切入点，为工程学提供了一个新的视角，同时也为普通读者和工程师架起了一座沟通的桥梁。尽管在学术深度上存在一定局限性，但 Petroski 的研究对理解技术失败的意义和推动工程学进步具有重要价值。



# 18. E. Kemp (1986) 对 Petroski 的*《To Engineer is Human》的评论*

### **文献标题：**

**Review of \*To Engineer is Human: The Role of Failure in Successful Design\***
 **作者：** E. Kemp
 **期刊：** *Technology & Culture*
 **出版时间：** 1986年
 **卷号与页码：** 第27卷第4期，第846-848页
 **链接：** [WorldCat 链接](https://bris.on.worldcat.org/oclc/7374892942)

------

### **核心内容概述**

E. Kemp 在这篇评论中对 Henry Petroski 的著作 *To Engineer is Human: The Role of Failure in Successful Design* 进行了全面评价。这本书的核心思想是将**失败**视为工程设计中不可或缺的一部分，并强调失败如何为工程实践带来改进和进步。Kemp 高度评价了 Petroski 的叙述风格、理论深度和对历史案例的巧妙运用，但也对其局限性提出了一些批评。

------

### **1. 书中核心观点**

#### **1.1 失败是设计成功的基础**

- Petroski 强调，**失败（Failure）**是工程设计中不可避免的现象，也是推动技术进步的关键。
  - 从失败中学习（Learning from Failure）：
    - 每一次失败都能揭示设计中的缺陷，并为未来改进提供方向。

#### **1.2 工程设计的哲学**

- 设计的迭代性（Iterative Nature of Design）：
  - 工程设计是一个试验-失败-优化的循环过程。
  - 没有完全“完美”的设计，每一个新设计都建立在过去失败的基础上。
- 工程的不确定性（Uncertainty in Engineering）：
  - 工程师常在不完全信息的情况下进行设计，容忍一定程度的失败是必然的。

#### **1.3 案例分析**

- Petroski 用生动的历史案例说明失败如何推动了技术和设计的改进：
  - 桥梁倒塌（Bridge Failures）：
    - 泰晤士河大桥的倒塌展示了材料和结构设计中的弱点。
  - 水坝灾难（Dam Disasters）：
    - 这些失败揭示了施工技术和材料科学的局限性。

------

### **2. Kemp 对 Petroski 的评价**

#### **2.1 对书中优点的认可**

1. **叙述的吸引力：**
   - Kemp 称赞 Petroski 的写作风格生动且易于理解，将复杂的技术问题转化为普通读者能够接受的语言。
2. **理论与实践的结合：**
   - Petroski 通过历史案例展示了理论在实际工程设计中的应用，使读者能够清晰地理解失败与改进之间的关系。
3. **对失败的重新定义：**
   - Petroski 提出了创新性观点，将失败视为工程设计中的正面力量，而非简单的错误或负面事件。

#### **2.2 对书中不足的批评**

1. **学术深度的局限性：**
   - Kemp 认为，Petroski 的叙述更偏向普及性科普作品，而在工程理论深度上有所欠缺。
   - 他建议加入更多对技术细节和数学模型的讨论，以吸引专业工程师。
2. **对成功案例的分析不足：**
   - 书中更多关注失败的案例，而对如何在设计中预防失败和确保成功的讨论较少。
3. **现代工程的覆盖不足：**
   - Kemp 指出，Petroski 的案例多集中于传统土木工程，对现代技术（如计算机科学或航空工程）的讨论有限。

------

### **3. 技术失败的关键名词中英对照**

| **中文** | **英文**           |
| -------- | ------------------ |
| 失败     | Failure            |
| 工程设计 | Engineering Design |
| 迭代性   | Iterative Nature   |
| 学习曲线 | Learning Curve     |
| 不确定性 | Uncertainty        |
| 材料科学 | Material Science   |
| 桥梁倒塌 | Bridge Failures    |
| 水坝灾难 | Dam Disasters      |

------

### **4. Petroski 的研究对技术失败主题的贡献**

#### **4.1 失败的正面作用**

- Petroski 的研究重新定义了失败在工程设计中的角色：
  - 失败不是终点，而是改进的起点。
  - 它促使工程师更加重视设计的细节和潜在问题。

#### **4.2 提供历史视角**

- 通过回顾历史案例，Petroski 展示了技术领域如何通过一系列失败逐步进步。

#### **4.3 普及工程学理念**

- 书中以通俗易懂的语言和生动案例，使普通读者更好地理解工程设计的复杂性和失败的重要性。

#### **4.4 激励未来研究**

- Kemp 认为，Petroski 的观点为研究现代技术领域（如人工智能或航空航天）的失败与改进提供了理论启示。

------

### **总结**

E. Kemp 的评论全面肯定了 Henry Petroski 在 *To Engineer is Human* 中对工程失败和设计哲学的深刻探讨。虽然在学术深度和案例多样性上存在局限性，但 Petroski 的研究通过历史案例重新定义了失败的意义，为工程师和普通读者提供了宝贵的反思工具。Kemp 认为，这本书在“技术失败”主题中具有重要的启示价值，并为未来研究提供了丰富的讨论空间。

# 19 D. Cliff & L. Northrop (2011) The Global Financial Markets: An Ultra-Large-Scale-Systems Perspective. 

### **文献标题：**

**The Global Financial Markets: An Ultra-Large-Scale-Systems Perspective**
 **作者：** D. Cliff & L. Northrop
 **项目：** Driver Review DR4, UK Government Office for Science Foresight Project *The Future of Computer Trading in Financial Markets*
 **出版时间：** 2011年

------

### **核心内容概述**

D. Cliff 和 L. Northrop 在这份报告中应用**超大规模系统（Ultra-Large-Scale Systems, ULS Systems）**的理论框架，分析全球金融市场的复杂性及其在计算机化交易时代的演化趋势。他们探讨了金融市场作为一个复杂技术系统的特征、风险与挑战，并提出了管理和设计这些系统的新方法。

------

### **1. 核心观点**

#### **1.1 超大规模系统的定义与特征**

- 超大规模系统（ULS Systems）

  ：由数量庞大且复杂的组件构成，具有以下特征：

  - **高复杂性（High Complexity）：** 系统中的交互关系难以完全理解和预测。
  - **分布性（Distributed Nature）：** 系统在全球范围内分布。
  - **多利益相关者（Multi-Stakeholder Environment）：** 不同利益方参与，目标多样化。
  - **持续演化（Continuous Evolution）：** 系统随着技术和环境变化不断升级。

#### **1.2 金融市场的超大规模系统特性**

- 金融市场的复杂性：
  - 由无数的参与者（如投资者、机构）、规则（如监管政策）和技术（如算法交易）构成。
- 算法交易的兴起：
  - 计算机交易系统在速度和规模上的优势，使得市场交易量激增。
  - **高频交易（High-Frequency Trading, HFT）** 是其典型代表，涉及微秒级的交易决策。
- 系统性风险（Systemic Risks）：
  - 紧密耦合和复杂交互可能导致单点失效（Single Point of Failure）演变为全系统崩溃。

------

### **2. 报告的关键问题与风险**

#### **2.1 不确定性与不可预测性**

- 系统中的交互关系过于复杂，可能产生意外行为或“黑天鹅事件”（Black Swan Events）。
- 算法交易的速度和复杂性使监管者难以及时发现和响应问题。

#### **2.2 网络效应与反馈回路**

- 系统中的正反馈机制可能放大市场波动：
  - **案例：2010年“闪电崩盘”（Flash Crash）**，算法交易导致市场瞬间暴跌。

#### **2.3 安全性与脆弱性**

- 随着技术的复杂化，系统受到网络攻击和技术故障的风险增加。

------

### **3. 管理与设计建议**

#### **3.1 系统设计原则**

- 模块化（Modularity）：
  - 将系统分解为独立模块，减少局部故障对全局的影响。
- 冗余性（Redundancy）：
  - 增加关键系统的备份，确保在故障情况下能够继续运行。

#### **3.2 数据驱动的监管**

- 使用实时数据分析工具，增强对市场行为的可视化能力，及时识别异常。
- **案例：** 实时监控高频交易活动，识别潜在系统性风险。

#### **3.3 加强全球合作**

- 金融市场的全球化要求跨国监管机构的协作，以应对系统性风险。

#### **3.4 伦理与透明性**

- 要求交易算法的透明性，确保它们的行为符合市场的公平性原则。

------

### **4. 技术失败的关键名词中英对照**

| **中文**     | **英文**                                |
| ------------ | --------------------------------------- |
| 超大规模系统 | Ultra-Large-Scale Systems (ULS Systems) |
| 高频交易     | High-Frequency Trading (HFT)            |
| 系统性风险   | Systemic Risks                          |
| 闪电崩盘     | Flash Crash                             |
| 黑天鹅事件   | Black Swan Events                       |
| 模块化       | Modularity                              |
| 冗余性       | Redundancy                              |
| 网络效应     | Network Effects                         |
| 反馈回路     | Feedback Loops                          |

------

### **5. 报告对技术失败主题的贡献**

#### **5.1 强调复杂系统的脆弱性**

- 报告揭示了全球金融市场作为技术驱动的复杂系统，其高复杂性和紧密耦合性使其容易发生技术失败。

#### **5.2 提供管理复杂系统的新方法**

- 报告提出的模块化和数据驱动监管等建议，为管理复杂技术系统提供了实践指导。

#### **5.3 借鉴跨领域经验**

- 通过将 ULS 系统理论应用于金融市场，报告为其他复杂系统（如航空、能源、医疗等）的风险管理提供了参考框架。

------

### **总结**

D. Cliff 和 L. Northrop 的报告通过超大规模系统的视角分析了全球金融市场的复杂性和风险，提出了具体的设计和管理建议。这份报告不仅为金融市场的监管和风险管理提供了重要参考，还对“技术失败”主题的研究具有重要的跨学科意义。

# 20. Diane Vaughan

### **视频标题：**

**Diane Vaughan Keynote Lecture: Dead Reckoning and Its Links to the Challenger Launch Decision**
 **链接：** [YouTube 视频链接](https://www.youtube.com/watch?v=uu0tPsb_Wnc&t=1845s) (1h27m11s)

------

### **核心内容概述**

Diane Vaughan 在这场演讲中分享了她正在撰写的新书 *Dead Reckoning: Air Traffic Control, System Effects, and Risk* 的核心内容，并将其与她关于 *The Challenger Launch Decision* 的研究进行了联系。Vaughan 强调了组织系统的成功与失败之间的对比，深入分析了航空交通控制作为一个复杂技术系统如何避免事故，以及这种系统性行为的管理和文化基础。

尽管视频质量不佳，演讲提供了对高风险技术系统管理的宝贵洞见。

------

### **1. 演讲的核心观点**

#### **1.1 航空交通控制系统的特点**

- Vaughan 将航空交通控制（Air Traffic Control, ATC）描述为一个**复杂且高风险的组织系统**，但它的运行表现出高度的可靠性和安全性。
- 她指出，ATC 系统通过成功的文化、程序和技术设计实现了“**系统性成功（Systemic Success）**”。

#### **1.2 系统效应与组织文化**

- 系统效应（System Effects）：
  - ATC 系统的成功来自于其各个部分的紧密协作，但同时保留了足够的灵活性来适应不确定性。
  - 系统中多层次的冗余设计和沟通机制是其可靠性的基础。
- 组织文化（Organizational Culture）：
  - Vaughan 强调，ATC 的文化鼓励信息共享和协作，而非隐藏问题。
  - 这种文化有助于快速解决潜在问题，从而避免灾难。

#### **1.3 将 ATC 与挑战者号发射决策联系起来**

- Vaughan 通过对比，说明 ATC 系统的成功机制如何与挑战者号的失败形成鲜明对比：
  - 挑战者号案例中的失败：
    - NASA 的组织文化偏差（如目标优先于安全）和沟通障碍（如忽视工程师的警告）导致了灾难。
  - ATC 的成功：
    - ATC 系统中的开放文化和多层防御机制有效避免了类似问题。

#### **1.4 风险与决策管理**

- Vaughan 强调，成功的高风险系统需要：
  - 透明的沟通机制。
    Transparent communication mechanism.
  - 冗余设计和灵活性。
    Redundant design and flexibility.
  - 对文化和制度性偏差的持续关注。
    Continued attention to cultural and institutional biases.

------

### **2. 主要案例分析**

#### **2.1 航空交通控制的成功**

- Vaughan 分享了多个案例，展示了航空交通控制系统如何在高度复杂和动态的环境中保持安全：
  - 应对突发状况时，控制员通过清晰的沟通和协调快速解决问题。
  - 冗余通信和监控系统确保了信息传递的准确性。

#### **2.2 挑战者号发射决策的失败**

- Vaughan 回顾了 NASA 的**偏差正常化（Normalization of Deviance）**，解释了如何逐渐接受风险，最终导致了灾难。
- 她指出，这种文化偏差是高风险组织中普遍存在的问题，但可以通过改进管理和文化加以克服。

------

### **3. 关键名词中英对照**

| **中文**     | **英文**                  |
| ------------ | ------------------------- |
| 航空交通控制 | Air Traffic Control (ATC) |
| 系统性成功   | Systemic Success          |
| 系统效应     | System Effects            |
| 偏差的正常化 | Normalization of Deviance |
| 组织文化     | Organizational Culture    |
| 冗余设计     | Redundancy Design         |
| 风险管理     | Risk Management           |

------

### **4. 演讲的洞见与启示**

#### **4.1 高风险系统的成功模式**

- Vaughan 提供了 ATC 的成功经验，说明复杂技术系统可以通过：
  - 开放文化和透明沟通机制减少事故风险。
  - 多层冗余设计确保系统的灵活性和恢复能力。

#### **4.2 对失败的反思**

- 她呼吁对 NASA 等失败案例的持续反思，以避免未来重蹈覆辙。

#### **4.3 文化在技术系统中的核心作用**

- Vaughan 强调，组织文化比技术设计本身更关键，因为文化决定了如何处理不确定性和风险。

#### **4.4 对未来研究的启发**

- 她的新书探讨了如何从成功的复杂系统中提取可应用于其他高风险领域的经验教训。

------

### **总结**

Diane Vaughan 的演讲通过分析航空交通控制系统的成功与挑战者号灾难的失败，为研究复杂技术系统的管理和文化提供了重要洞见。她提出，系统的成功依赖于开放的文化、灵活的设计和对风险的持续关注。这场演讲不仅为她的新书 *Dead Reckoning* 提供了理论基础，也为高风险技术领域的实践和研究提供了宝贵参考。